{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Short Lab: Visualizing Data Mining Techniques\n",
        "\n",
        "This lab is based on **Lecture 7.1 (Data Mining)** and **Lecture 7.2 (Decision Trees)**.\n",
        "\n",
        "You will practice and visualize:\n",
        "- Classification and clustering (Data Mining core tasks)\n",
        "- Decision Tree splits with **Gini** vs **Entropy**\n",
        "- Feature importance and model quality\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup\n",
        "\n",
        "If you run this in a fresh environment, install dependencies from `requirements.txt` first.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    confusion_matrix,\n",
        "    ConfusionMatrixDisplay,\n",
        "    adjusted_rand_score,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "sns.set_theme(style=\"whitegrid\", context=\"notebook\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Open-Source Dataset\n",
        "\n",
        "We use the **Wine** dataset from scikit-learn (originally from UCI ML Repository).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wine = load_wine(as_frame=True)\n",
        "df = wine.frame.copy()\n",
        "df[\"target_name\"] = df[\"target\"].map(lambda x: wine.target_names[x])\n",
        "\n",
        "print(\"Shape:\", df.shape)\n",
        "print(\"Classes:\", list(wine.target_names))\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Quick EDA (Data Visualization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Class distribution\n",
        "sns.countplot(data=df, x=\"target_name\", hue=\"target_name\", palette=\"Set2\", ax=axes[0], legend=False)\n",
        "axes[0].set_title(\"Class Distribution\")\n",
        "axes[0].set_xlabel(\"Wine class\")\n",
        "axes[0].set_ylabel(\"Count\")\n",
        "\n",
        "# Correlation heatmap\n",
        "corr = df.drop(columns=[\"target_name\"]).corr(numeric_only=True)\n",
        "sns.heatmap(corr, cmap=\"coolwarm\", center=0, ax=axes[1])\n",
        "axes[1].set_title(\"Feature Correlation Heatmap\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_features = [\"alcohol\", \"malic_acid\", \"flavanoids\", \"color_intensity\", \"target_name\"]\n",
        "sns.pairplot(df[selected_features], hue=\"target_name\", corner=True, diag_kind=\"kde\")\n",
        "plt.suptitle(\"Feature Relationships by Class\", y=1.02)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Clustering Visualization (K-Means)\n",
        "\n",
        "Even without labels, clustering can find natural groups in data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df.drop(columns=[\"target\", \"target_name\"])\n",
        "y = df[\"target\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=RANDOM_STATE, n_init=20)\n",
        "cluster_labels = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Reduce to 2D only for visualization\n",
        "pca = PCA(n_components=2, random_state=RANDOM_STATE)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "ari = adjusted_rand_score(y, cluster_labels)\n",
        "print(f\"Adjusted Rand Index (clusters vs true classes): {ari:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
        "\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=df[\"target_name\"], palette=\"Set2\", ax=axes[0])\n",
        "axes[0].set_title(\"True Classes (PCA Projection)\")\n",
        "axes[0].set_xlabel(\"PC1\")\n",
        "axes[0].set_ylabel(\"PC2\")\n",
        "\n",
        "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=cluster_labels, palette=\"tab10\", ax=axes[1])\n",
        "axes[1].set_title(\"K-Means Clusters (PCA Projection)\")\n",
        "axes[1].set_xlabel(\"PC1\")\n",
        "axes[1].set_ylabel(\"PC2\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Decision Trees: Gini vs Entropy\n",
        "\n",
        "From Lecture 7.2: compare split criteria and visualize the learned tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "def train_and_eval(criterion, depth=4):\n",
        "    model = DecisionTreeClassifier(\n",
        "        criterion=criterion,\n",
        "        max_depth=depth,\n",
        "        random_state=RANDOM_STATE,\n",
        "    )\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test)\n",
        "    acc = accuracy_score(y_test, preds)\n",
        "    return model, preds, acc\n",
        "\n",
        "model_gini, preds_gini, acc_gini = train_and_eval(\"gini\", depth=4)\n",
        "model_entropy, preds_entropy, acc_entropy = train_and_eval(\"entropy\", depth=4)\n",
        "\n",
        "print(f\"Gini accuracy:    {acc_gini:.3f}\")\n",
        "print(f\"Entropy accuracy: {acc_entropy:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, preds_gini)).plot(ax=axes[0], colorbar=False)\n",
        "axes[0].set_title(\"Confusion Matrix: Gini\")\n",
        "\n",
        "ConfusionMatrixDisplay(confusion_matrix(y_test, preds_entropy)).plot(ax=axes[1], colorbar=False)\n",
        "axes[1].set_title(\"Confusion Matrix: Entropy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Classification report (Entropy):\")\n",
        "print(classification_report(y_test, preds_entropy, target_names=wine.target_names))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "plot_tree(\n",
        "    model_entropy,\n",
        "    feature_names=X.columns,\n",
        "    class_names=wine.target_names,\n",
        "    filled=True,\n",
        "    rounded=True,\n",
        "    fontsize=9,\n",
        ")\n",
        "plt.title(\"Decision Tree Visualization (Entropy, max_depth=4)\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Complexity and Overfitting\n",
        "\n",
        "Observe how tree depth changes train/test accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "depths = range(1, 11)\n",
        "train_scores_gini, test_scores_gini = [], []\n",
        "train_scores_entropy, test_scores_entropy = [], []\n",
        "\n",
        "for d in depths:\n",
        "    m_g = DecisionTreeClassifier(criterion=\"gini\", max_depth=d, random_state=RANDOM_STATE)\n",
        "    m_e = DecisionTreeClassifier(criterion=\"entropy\", max_depth=d, random_state=RANDOM_STATE)\n",
        "\n",
        "    m_g.fit(X_train, y_train)\n",
        "    m_e.fit(X_train, y_train)\n",
        "\n",
        "    train_scores_gini.append(m_g.score(X_train, y_train))\n",
        "    test_scores_gini.append(m_g.score(X_test, y_test))\n",
        "\n",
        "    train_scores_entropy.append(m_e.score(X_train, y_train))\n",
        "    test_scores_entropy.append(m_e.score(X_test, y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(depths, train_scores_gini, marker=\"o\", label=\"Train (Gini)\")\n",
        "plt.plot(depths, test_scores_gini, marker=\"o\", label=\"Test (Gini)\")\n",
        "plt.plot(depths, train_scores_entropy, marker=\"s\", label=\"Train (Entropy)\")\n",
        "plt.plot(depths, test_scores_entropy, marker=\"s\", label=\"Test (Entropy)\")\n",
        "plt.xlabel(\"Max depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Decision Tree Depth vs Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Feature Importance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "importance = pd.Series(model_entropy.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=importance.values, y=importance.index, palette=\"viridis\")\n",
        "plt.title(\"Feature Importance (Decision Tree, Entropy)\")\n",
        "plt.xlabel(\"Importance\")\n",
        "plt.ylabel(\"Feature\")\n",
        "plt.show()\n",
        "\n",
        "importance.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Missing Data and Imputation Techniques\n",
        "\n",
        "Real datasets often have missing values. Below we simulate missingness and compare common imputation techniques.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simulate missing values in 10% of feature cells\n",
        "missing_rate = 0.10\n",
        "rng = np.random.default_rng(RANDOM_STATE)\n",
        "\n",
        "X_missing = X.copy()\n",
        "mask = rng.random(X_missing.shape) < missing_rate\n",
        "X_missing = X_missing.mask(mask)\n",
        "\n",
        "missing_total = int(X_missing.isna().sum().sum())\n",
        "missing_pct = 100 * missing_total / X_missing.size\n",
        "print(f\"Missing cells: {missing_total} ({missing_pct:.1f}%)\")\n",
        "\n",
        "X_missing.isna().mean().sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_miss, X_test_miss, y_train_miss, y_test_miss = train_test_split(\n",
        "    X_missing, y, test_size=0.25, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "\n",
        "imputers = {\n",
        "    \"mean\": SimpleImputer(strategy=\"mean\"),\n",
        "    \"median\": SimpleImputer(strategy=\"median\"),\n",
        "    \"most_frequent\": SimpleImputer(strategy=\"most_frequent\"),\n",
        "    \"knn\": KNNImputer(n_neighbors=5),\n",
        "}\n",
        "\n",
        "rows = []\n",
        "for name, imputer in imputers.items():\n",
        "    X_train_imp = imputer.fit_transform(X_train_miss)\n",
        "    X_test_imp = imputer.transform(X_test_miss)\n",
        "\n",
        "    clf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=4, random_state=RANDOM_STATE)\n",
        "    clf.fit(X_train_imp, y_train_miss)\n",
        "    preds = clf.predict(X_test_imp)\n",
        "\n",
        "    rows.append(\n",
        "        {\n",
        "            \"imputer\": name,\n",
        "            \"accuracy\": accuracy_score(y_test_miss, preds),\n",
        "            \"remaining_missing_train\": int(np.isnan(X_train_imp).sum()),\n",
        "            \"remaining_missing_test\": int(np.isnan(X_test_imp).sum()),\n",
        "        }\n",
        "    )\n",
        "\n",
        "imputation_results = pd.DataFrame(rows).sort_values(\"accuracy\", ascending=False)\n",
        "imputation_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "feature = \"flavanoids\"\n",
        "feature_idx = X.columns.get_loc(feature)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.kdeplot(X[feature], label=\"original (no missing)\", linewidth=2)\n",
        "\n",
        "for name, imputer in imputers.items():\n",
        "    X_imp_full = imputer.fit_transform(X_missing)\n",
        "    sns.kdeplot(X_imp_full[:, feature_idx], label=f\"imputed: {name}\", alpha=0.85)\n",
        "\n",
        "plt.title(f\"Distribution Comparison for '{feature}'\")\n",
        "plt.xlabel(feature)\n",
        "plt.ylabel(\"Density\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Variant Tasks for 10 Students\n",
        "\n",
        "Assign one variant to each student. Every variant includes a few tasks to complete.\n",
        "\n",
        "### Variant 1\n",
        "1. Run K-Means with `n_clusters=2`, `3`, and `4`.\n",
        "2. Compare ARI values in a small table.\n",
        "3. Explain which `k` works best and why.\n",
        "\n",
        "### Variant 2\n",
        "1. Keep `k=3` and test K-Means with and without `StandardScaler`.\n",
        "2. Compare ARI and PCA cluster plots.\n",
        "3. Write a short conclusion about scaling impact.\n",
        "\n",
        "### Variant 3\n",
        "1. Build decision trees with `max_depth=2`, `4`, and `8` using `criterion='gini'`.\n",
        "2. Compare train/test accuracy in one plot.\n",
        "3. Explain where overfitting starts.\n",
        "\n",
        "### Variant 4\n",
        "1. Build decision trees with `criterion='gini'` and `criterion='entropy'` at `max_depth=4`.\n",
        "2. Compare confusion matrices and macro F1.\n",
        "3. Decide which criterion is better on this dataset.\n",
        "\n",
        "### Variant 5\n",
        "1. Simulate missingness at `10%` and `25%`.\n",
        "2. Compare `SimpleImputer(mean)` and `SimpleImputer(median)`.\n",
        "3. Report which method is more stable as missingness increases.\n",
        "\n",
        "### Variant 6\n",
        "1. Simulate missingness at `15%`.\n",
        "2. Compare `SimpleImputer(most_frequent)` vs `KNNImputer`.\n",
        "3. Evaluate both using decision-tree test accuracy.\n",
        "\n",
        "### Variant 7\n",
        "1. After imputation, compare feature distributions for two features (not only `flavanoids`).\n",
        "2. Show KDE plots for original vs imputed data.\n",
        "3. Identify which feature is most sensitive to imputation choice.\n",
        "\n",
        "### Variant 8\n",
        "1. Train an entropy tree (`max_depth=4`).\n",
        "2. Extract top-5 most important features.\n",
        "3. Remove the top-1 feature, retrain, and compare accuracy.\n",
        "\n",
        "### Variant 9\n",
        "1. Add 5-fold cross-validation for decision trees (`gini` and `entropy`).\n",
        "2. Report mean and std accuracy for each criterion.\n",
        "3. Compare CV results with the single train/test split.\n",
        "\n",
        "### Variant 10\n",
        "1. Create one final comparison table with the best result from clustering, tree tuning, and imputation.\n",
        "2. Select your recommended pipeline for this dataset.\n",
        "3. Provide a 5-7 sentence final summary for a non-technical audience.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}